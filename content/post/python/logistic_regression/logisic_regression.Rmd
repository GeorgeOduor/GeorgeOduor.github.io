---
title: "Logistic Regression with Python Implementation"
author: "George Oduor"
date:  "2022-08-29T00:00:00Z"
categories: ["Python"]
tags: ["Python", "Machine Learning","Supervised Learning","Classification","Regression"]
img : 'oop.png'
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

1. What is logistic regression?
    - Applications in real life.
1. Brief Mathematical background
1. Implementation with python
1. Model Validation
1. Model Improvement
1. Conclusion


## What is logistic regression?

Logistic regression is a classification method that uses regression technique as an internal component.This model works by estimating an unknown probability _p_ for any given liner combination of the independent variables.

__Binary Logistic regression__:This is when the dependent variable is presented in two categories eg _win vs good_ in the case of credit risk prediction

__Multinomial Logistic Regression__ : This is applicable when the dependent variable is presented in more than 2 categories eg _married,single,divorced or widowed_.

In this blog,I am going to focus on _binary_ variant.

## Brief Mathematical background

Mathematically,a set of explanatory variables is used to predict a _logit transformation_ of the dependent variable.Taking the credit risk scenario above,say the outcomes good and bad are marked 0 and 1 respectively.In most instances the positive class is labeled 1 and the negative class is marked 0.

mathematically this is represented as :

$$p(good) = p$$
$$p(bad) = 1-q $$

__Logistic Model__

In linear regression, a linear function of indepenednt variables shown below is used to successfully estimate the dependent variables.

$$p(x) = \beta_0+\beta_1X$$
Logistic regression estimates range form 0 to 1 depending on the probability of the outcome.In order to get this, a logistic function is therefore used,ie

$$p(X) = \frac{e^{\beta_0+\beta_1X}}{1+e^{\beta_0+\beta_1X}}$$
The model above is fitted using _maximum likelihood method_.This equation can be re written as :
$$\frac{p(X)}{1-p(X)} = e^{\beta_0+\beta_1X}$$
From the above $p(X)/(1-p(X))$ equals the odds.The higher the odds the higher the probability of the positive class.

If we take the logarithm of both sides:

$$log(\frac{p(X)}{1-p(X)}) = \beta_0+\beta_1X $$

The left hand side gives the _log odds_ or the logit value.

__Coefficient Estimation.__

The coefficients are estimated using maximum likelihood technique.This is beyond the scope of this blog.

## Implementation with Python

```{python}
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

from sklearn.datasets import make_classification
```

### Dataset

For demonstration purposes,we are going to generate synthetic data using sklearn.

```{python}
X,y = make_classification(n_samples=10000, n_features=5, n_redundant=0, n_classes=2, n_clusters_per_class=1, class_sep=1.0, random_state=0)
```

The a above line of code creates 10,000 records/observations,5 independent variables and 2 classes.

```{python}
X.shape
```

```{python}
X = pd.DataFrame(X,columns=[f"var_{i}" for i in range(1,6)])
X.head()
```

## Exploratory Data Analysis

Its important to investigate the distribution of every variable.This helps to spot any anomalies like outliers,missing values or skewness.Such factors greatly affect the output of models.

### Target/Dependent Variable

```{python}
target_summary = pd.Series(y).value_counts(normalize=True)

plt.pie(target_summary.values,labels=[1,0],autopct='%1.1f%%', startangle=30)
plt.title("Distribution of the Target Variable")
plt.show()
```

__Observations and Inferences__

- The classes have a slight difference in proportion,meaning we have a fairly balanced class.

### Independent/Explanatory Variables

```{python histograms}
fig = plt.figure(1)
for i in range(5):
  plt.subplot(1,5,i+1)
  plt.hist(X[X.columns[i]])
  plt.xlabel(f"{X.columns[i]}")
fig.tight_layout()
plt.show()

```

__Observations and Inferences__

- All the 5 variables generated appear to be normally distributed shown by the bell shaped histograms.This is one characteristic that is favorable for logistic regression.

__Variable Correlation__

```{python}
p1 = sns.heatmap(X.corr(),annot=True,fmt='.2f',cmap="viridis",linewidth=.5,vmax=.3, square=True)
p1
```


__Observations and Inferences__

- From the correlation heat map above,we can observe that all the variables are not highly correlated.This is a good property expected for independent variables.


__Relationship between the target variable and
```{r}
34
```


